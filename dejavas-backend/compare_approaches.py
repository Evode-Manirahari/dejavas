#!/usr/bin/env python3
"""
Dejavas Approach Comparison Demo

This script compares different LLM integration approaches:
1. Direct LLM (Claude/Gemini) - Fast, simple
2. LangGraph - Complex, realistic interactions
3. Hybrid - Best of both worlds
"""

import asyncio
import time
from typing import Dict, Any

from hybrid_simulation import (
    HybridSimulator, 
    SimulationMode, 
    SimulationRequest, 
    SimulationComparison
)

async def compare_llm_approaches():
    """Compare different LLM approaches"""
    
    print("üî¨ Dejavas LLM Approach Comparison")
    print("=" * 60)
    
    # Sample content for testing
    test_content = {
        "features": [
            {
                "title": "AI-Powered Market Analysis",
                "description": "Get instant market intelligence on any product using AI agents that think like real customers, competitors, and influencers."
            },
            {
                "title": "Browser Extension Integration", 
                "description": "Works everywhere like Grammarly - analyze any webpage instantly without copy-pasting."
            },
            {
                "title": "Competitive Intelligence",
                "description": "Understand how competitors would react to your features and identify market opportunities."
            }
        ]
    }
    
    agent_config = {
        'customer_percentage': 60,
        'competitor_percentage': 20,
        'influencer_percentage': 10,
        'internal_team_percentage': 10
    }
    
    print("üìã Test Content:")
    for i, feature in enumerate(test_content['features'], 1):
        print(f"  {i}. {feature['title']}")
    
    print(f"\nü§ñ Agent Configuration:")
    for agent_type, percentage in agent_config.items():
        print(f"  {agent_type}: {percentage}%")
    
    # Test different approaches
    approaches = [
        ("Quick Analysis (Direct LLM)", SimulationMode.QUICK_ANALYSIS),
        ("Standard Simulation", SimulationMode.STANDARD_SIMULATION),
        ("Complex War Room (LangGraph)", SimulationMode.COMPLEX_WAR_ROOM)
    ]
    
    results = {}
    
    for approach_name, mode in approaches:
        print(f"\nüß™ Testing: {approach_name}")
        print("-" * 40)
        
        start_time = time.time()
        
        try:
            simulator = HybridSimulator()
            request = SimulationRequest(
                content=test_content,
                mode=mode,
                agent_config=agent_config
            )
            
            result = await simulator.simulate(request)
            
            end_time = time.time()
            duration = end_time - start_time
            
            results[approach_name] = {
                'adoption_score': result.get('adoption_score', 0),
                'analysis_time': f"{duration:.2f}s",
                'mode_used': result.get('mode', 'unknown'),
                'objections_count': len(result.get('top_objections', [])),
                'fixes_count': len(result.get('must_fix', [])),
                'success': True
            }
            
            print(f"‚úÖ Success!")
            print(f"   Adoption Score: {result.get('adoption_score', 0):.1f}%")
            print(f"   Analysis Time: {duration:.2f}s")
            print(f"   Objections: {len(result.get('top_objections', []))}")
            print(f"   Must Fix: {len(result.get('must_fix', []))}")
            
        except Exception as e:
            results[approach_name] = {
                'error': str(e),
                'success': False
            }
            print(f"‚ùå Failed: {e}")
    
    # Display comparison
    print(f"\nüìä Approach Comparison")
    print("=" * 60)
    
    print(f"{'Approach':<25} {'Score':<8} {'Time':<8} {'Objections':<12} {'Fixes':<8}")
    print("-" * 60)
    
    for approach_name, result in results.items():
        if result.get('success'):
            print(f"{approach_name:<25} {result['adoption_score']:<8.1f} {result['analysis_time']:<8} {result['objections_count']:<12} {result['fixes_count']:<8}")
        else:
            print(f"{approach_name:<25} {'ERROR':<8} {'N/A':<8} {'N/A':<12} {'N/A':<8}")
    
    # Recommendations
    print(f"\nüí° Recommendations")
    print("=" * 60)
    
    successful_results = {k: v for k, v in results.items() if v.get('success')}
    
    if len(successful_results) > 1:
        # Find fastest
        fastest = min(successful_results.items(), key=lambda x: float(x[1]['analysis_time'].replace('s', '')))
        print(f"üöÄ Fastest: {fastest[0]} ({fastest[1]['analysis_time']})")
        
        # Find highest score
        highest_score = max(successful_results.items(), key=lambda x: x[1]['adoption_score'])
        print(f"üèÜ Highest Score: {highest_score[0]} ({highest_score[1]['adoption_score']:.1f}%)")
        
        # Find most insights
        most_insights = max(successful_results.items(), key=lambda x: x[1]['objections_count'] + x[1]['fixes_count'])
        total_insights = most_insights[1]['objections_count'] + most_insights[1]['fixes_count']
        print(f"üí° Most Insights: {most_insights[0]} ({total_insights} total)")
    
    print(f"\nüéØ Use Case Recommendations:")
    print(f"‚Ä¢ Quick Analysis: Content review, initial screening, cost-sensitive scenarios")
    print(f"‚Ä¢ Standard Simulation: Product launches, feature validation, balanced analysis")
    print(f"‚Ä¢ Complex War Room: Strategic decisions, competitive analysis, high-stakes scenarios")

async def demo_llm_model_comparison():
    """Demo different LLM models (Claude vs Gemini vs GPT)"""
    
    print(f"\nü§ñ LLM Model Comparison")
    print("=" * 60)
    
    # Sample prompt for testing
    test_prompt = """
    Analyze this product feature from a customer perspective:
    
    Feature: AI-Powered Market Analysis
    Description: Get instant market intelligence on any product using AI agents that think like real customers.
    
    Provide:
    1. Adoption likelihood (0-100%)
    2. Top 3 objections
    3. Top 3 suggestions for improvement
    """
    
    models = [
        ("GPT-4", "gpt-4-turbo-preview"),
        ("Claude-3", "claude-3-sonnet-20240229"),
        ("Gemini Pro", "gemini-pro")
    ]
    
    print("Testing different LLM models with the same prompt...")
    
    for model_name, model_id in models:
        print(f"\nüß™ Testing {model_name}...")
        
        try:
            # This would require setting up different model clients
            # For now, we'll simulate the comparison
            start_time = time.time()
            
            # Simulate different response characteristics
            if "gpt" in model_id.lower():
                response_time = 2.5
                quality_score = 85
                cost = "medium"
            elif "claude" in model_id.lower():
                response_time = 3.2
                quality_score = 90
                cost = "high"
            elif "gemini" in model_id.lower():
                response_time = 1.8
                quality_score = 80
                cost = "low"
            else:
                response_time = 2.0
                quality_score = 75
                cost = "medium"
            
            await asyncio.sleep(0.1)  # Simulate processing
            
            print(f"   Response Time: {response_time:.1f}s")
            print(f"   Quality Score: {quality_score}/100")
            print(f"   Cost: {cost}")
            
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
    
    print(f"\nüí° Model Recommendations:")
    print(f"‚Ä¢ GPT-4: Best overall performance, good for complex analysis")
    print(f"‚Ä¢ Claude-3: Excellent reasoning, best for strategic insights")
    print(f"‚Ä¢ Gemini Pro: Fast and cost-effective, good for quick analysis")

async def demo_hybrid_approach():
    """Demo the hybrid approach in action"""
    
    print(f"\nüîÑ Hybrid Approach Demo")
    print("=" * 60)
    
    # Test different content types
    test_cases = [
        {
            "name": "Simple Content Analysis",
            "content": {"text": "Our new AI tool helps marketers analyze content."},
            "expected_mode": "quick_analysis"
        },
        {
            "name": "Product Feature Analysis", 
            "content": {
                "features": [
                    {"title": "Feature 1", "description": "Description 1"},
                    {"title": "Feature 2", "description": "Description 2"}
                ]
            },
            "expected_mode": "standard_simulation"
        },
        {
            "name": "Complex Competitive Analysis",
            "content": {
                "features": [
                    {"title": "Feature 1", "description": "Description 1"},
                    {"title": "Feature 2", "description": "Description 2"},
                    {"title": "Feature 3", "description": "Description 3"},
                    {"title": "Feature 4", "description": "Description 4"}
                ],
                "competitor_analysis": True,
                "market_positioning": True
            },
            "expected_mode": "war_room"
        }
    ]
    
    simulator = HybridSimulator()
    
    for test_case in test_cases:
        print(f"\nüß™ {test_case['name']}")
        print("-" * 40)
        
        request = SimulationRequest(
            content=test_case['content'],
            mode=SimulationMode.STANDARD_SIMULATION,  # Let router decide
            agent_config={'customer_percentage': 100}
        )
        
        try:
            result = await simulator.simulate(request)
            actual_mode = result.get('mode', 'unknown')
            expected_mode = test_case['expected_mode']
            
            print(f"Expected Mode: {expected_mode}")
            print(f"Actual Mode: {actual_mode}")
            print(f"Adoption Score: {result.get('adoption_score', 0):.1f}%")
            print(f"Analysis Time: {result.get('analysis_time', 'unknown')}")
            
            if actual_mode == expected_mode:
                print("‚úÖ Correctly routed!")
            else:
                print("‚ö†Ô∏è Different routing than expected")
                
        except Exception as e:
            print(f"‚ùå Error: {e}")

async def main():
    """Run all comparison demos"""
    
    print("üî¨ Dejavas LLM Approach Comparison Demo")
    print("=" * 70)
    
    await compare_llm_approaches()
    await demo_llm_model_comparison()
    await demo_hybrid_approach()
    
    print(f"\nüéØ Summary & Recommendations")
    print("=" * 70)
    print("""
    **Best Approach for Dejavas:**
    
    1. **MVP Phase (Current)**: Direct LLM with GPT-4
       - Fast, reliable, cost-effective
       - Perfect for content analysis
       - Easy to debug and iterate
    
    2. **Growth Phase**: Hybrid Approach
       - Quick analysis for simple cases
       - Standard simulation for features
       - LangGraph for complex scenarios
    
    3. **Scale Phase**: Full LangGraph
       - True multi-agent interactions
       - Complex market simulations
       - Advanced competitive analysis
    
    **Model Recommendations:**
    - GPT-4: Best overall for Dejavas use case
    - Claude-3: Excellent for strategic insights
    - Gemini Pro: Good for cost-sensitive scenarios
    
    **Implementation Strategy:**
    1. Start with current direct LLM approach
    2. Add hybrid routing for optimization
    3. Implement LangGraph for premium features
    """)

if __name__ == "__main__":
    asyncio.run(main())
